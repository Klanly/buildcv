{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pnnx_torch12.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# patch"
      ],
      "metadata": {
        "id": "Ijlxu2r6pkPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/nopy.patch\n",
        "diff -Nuarp vision-0.11.1/cmake/TorchVisionConfig.cmake.in vision-0.11.1.no-python/cmake/TorchVisionConfig.cmake.in\n",
        "--- vision-0.11.1/cmake/TorchVisionConfig.cmake.in\t2021-10-21 09:16:22.000000000 +0800\n",
        "+++ vision-0.11.1.no-python/cmake/TorchVisionConfig.cmake.in\t2022-04-28 16:34:33.810073013 +0800\n",
        "@@ -28,11 +28,8 @@ include(\"${CMAKE_CURRENT_LIST_DIR}/${PN}\n",
        " if(NOT TARGET torch_library)\n",
        " find_package(Torch REQUIRED)\n",
        " endif()\n",
        "-if(NOT TARGET Python3::Python)\n",
        "-find_package(Python3 COMPONENTS Development)\n",
        "-endif()\n",
        " \n",
        "-set_target_properties(TorchVision::TorchVision PROPERTIES INTERFACE_INCLUDE_DIRECTORIES \"${${PN}_INCLUDE_DIR}\" INTERFACE_LINK_LIBRARIES \"torch;Python3::Python\" )\n",
        "+set_target_properties(TorchVision::TorchVision PROPERTIES INTERFACE_INCLUDE_DIRECTORIES \"${${PN}_INCLUDE_DIR}\" INTERFACE_LINK_LIBRARIES \"torch\" )\n",
        " \n",
        " \n",
        " if(@WITH_CUDA@)"
      ],
      "metadata": {
        "id": "D_QB3QNtpnMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/ops.patch\n",
        "diff -Nuarp vision-0.11.1/CMakeLists.txt vision-0.11.1-ops-only/CMakeLists.txt\n",
        "--- vision-0.11.1/CMakeLists.txt\t2021-10-22 00:16:22.000000000 +0800\n",
        "+++ vision-0.11.1-ops-only/CMakeLists.txt\t2022-01-05 14:05:26.227901325 +0800\n",
        "@@ -12,11 +12,7 @@ if(WITH_CUDA)\n",
        "   set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr\")\n",
        " endif()\n",
        " \n",
        "-find_package(Python3 COMPONENTS Development)\n",
        "-\n",
        " find_package(Torch REQUIRED)\n",
        "-find_package(PNG REQUIRED)\n",
        "-find_package(JPEG REQUIRED)\n",
        " \n",
        " function(CUDA_CONVERT_FLAGS EXISTING_TARGET)\n",
        "     get_property(old_flags TARGET ${EXISTING_TARGET} PROPERTY INTERFACE_COMPILE_OPTIONS)\n",
        "@@ -60,8 +56,8 @@ include(GNUInstallDirs)\n",
        " include(CMakePackageConfigHelpers)\n",
        " \n",
        " set(TVCPP torchvision/csrc)\n",
        "-list(APPEND ALLOW_LISTED ${TVCPP} ${TVCPP}/io/image ${TVCPP}/io/image/cpu ${TVCPP}/models ${TVCPP}/ops\n",
        "-  ${TVCPP}/ops/autograd ${TVCPP}/ops/cpu ${TVCPP}/io/image/cuda)\n",
        "+list(APPEND ALLOW_LISTED ${TVCPP} ${TVCPP}/ops\n",
        "+  ${TVCPP}/ops/autograd ${TVCPP}/ops/cpu)\n",
        " if(WITH_CUDA)\n",
        "     list(APPEND ALLOW_LISTED ${TVCPP}/ops/cuda ${TVCPP}/ops/autocast)\n",
        " endif()\n",
        "@@ -71,7 +67,7 @@ FOREACH(DIR ${ALLOW_LISTED})\n",
        " ENDFOREACH()\n",
        " \n",
        " add_library(${PROJECT_NAME} SHARED ${ALL_SOURCES})\n",
        "-target_link_libraries(${PROJECT_NAME} PRIVATE ${TORCH_LIBRARIES} ${PNG_LIBRARY} ${JPEG_LIBRARIES} Python3::Python)\n",
        "+target_link_libraries(${PROJECT_NAME} PRIVATE ${TORCH_LIBRARIES})\n",
        " set_target_properties(${PROJECT_NAME} PROPERTIES\n",
        "   EXPORT_NAME TorchVision\n",
        "   INSTALL_RPATH ${TORCH_INSTALL_PREFIX}/lib)\n",
        "diff -Nuarp vision-0.11.1/torchvision/csrc/vision.cpp vision-0.11.1-ops-only/torchvision/csrc/vision.cpp\n",
        "--- vision-0.11.1/torchvision/csrc/vision.cpp\t2021-10-22 00:16:22.000000000 +0800\n",
        "+++ vision-0.11.1-ops-only/torchvision/csrc/vision.cpp\t2022-01-05 14:07:11.003900353 +0800\n",
        "@@ -1,8 +1,5 @@\n",
        " #include \"vision.h\"\n",
        " \n",
        "-#ifndef MOBILE\n",
        "-#include <Python.h>\n",
        "-#endif\n",
        " #include <torch/library.h>\n",
        " \n",
        " #ifdef WITH_CUDA\n",
        "@@ -12,15 +9,6 @@\n",
        " #include <hip/hip_runtime.h>\n",
        " #endif\n",
        " \n",
        "-// If we are in a Windows environment, we need to define\n",
        "-// initialization functions for the _custom_ops extension\n",
        "-#ifdef _WIN32\n",
        "-PyMODINIT_FUNC PyInit__C(void) {\n",
        "-  // No need to do anything.\n",
        "-  return NULL;\n",
        "-}\n",
        "-#endif\n",
        "-\n",
        " namespace vision {\n",
        " int64_t cuda_version() {\n",
        " #ifdef WITH_CUDA"
      ],
      "metadata": {
        "id": "ss1JNwyupthW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新增區段"
      ],
      "metadata": {
        "id": "zGDWT7S1pqP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O libtorch.zip https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-1.12.0%2Bcpu.zip\n",
        "!unzip -q libtorch.zip"
      ],
      "metadata": {
        "id": "C52bCBpxoL_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torchvis"
      ],
      "metadata": {
        "id": "BhSY3ca0oJvt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe69XrOol-_E"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/pytorch/vision/archive/refs/tags/v0.13.0.zip -O vision.zip\n",
        "!unzip -q vision.zip\n",
        "%cd /content/vision-0.13.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# patch_raw torchvis"
      ],
      "metadata": {
        "id": "3tbs4pGjs2LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CMakeLists.txt\n",
        "cmake_minimum_required(VERSION 3.12)\n",
        "project(torchvision)\n",
        "set(CMAKE_CXX_STANDARD 14)\n",
        "file(STRINGS version.txt TORCHVISION_VERSION)\n",
        "\n",
        "option(WITH_CUDA \"Enable CUDA support\" OFF)\n",
        "option(USE_PYTHON \"Link to Python when building\" OFF)\n",
        "\n",
        "if(WITH_CUDA)\n",
        "  enable_language(CUDA)\n",
        "  add_definitions(-D__CUDA_NO_HALF_OPERATORS__)\n",
        "  add_definitions(-DWITH_CUDA)\n",
        "  set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr\")\n",
        "  # CUDA-11.x can not be compiled using C++14 standard on Windows\n",
        "  string(REGEX MATCH \"^[0-9]+\" CUDA_MAJOR ${CMAKE_CUDA_COMPILER_VERSION})\n",
        "  if(${CUDA_MAJOR} GREATER 10 AND MSVC)\n",
        "    set(CMAKE_CXX_STANDARD 17)\n",
        "  endif()\n",
        "endif()\n",
        "\n",
        "\n",
        "\n",
        "find_package(Torch REQUIRED)\n",
        "\n",
        "add_definitions(-DJPEG_FOUND)\n",
        "add_definitions(-DPNG_FOUND)\n",
        "\n",
        "function(CUDA_CONVERT_FLAGS EXISTING_TARGET)\n",
        "    get_property(old_flags TARGET ${EXISTING_TARGET} PROPERTY INTERFACE_COMPILE_OPTIONS)\n",
        "    if(NOT \"${old_flags}\" STREQUAL \"\")\n",
        "        string(REPLACE \";\" \",\" CUDA_flags \"${old_flags}\")\n",
        "        set_property(TARGET ${EXISTING_TARGET} PROPERTY INTERFACE_COMPILE_OPTIONS\n",
        "            \"$<$<BUILD_INTERFACE:$<COMPILE_LANGUAGE:CXX>>:${old_flags}>$<$<BUILD_INTERFACE:$<COMPILE_LANGUAGE:CUDA>>:-Xcompiler=${CUDA_flags}>\"\n",
        "            )\n",
        "    endif()\n",
        "endfunction()\n",
        "\n",
        "if(MSVC)\n",
        "  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /wd4819\")\n",
        "  if(WITH_CUDA)\n",
        "    set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -Xcompiler=/wd4819\")\n",
        "    foreach(diag cc_clobber_ignored integer_sign_change useless_using_declaration\n",
        "      set_but_not_used field_without_dll_interface\n",
        "      base_class_has_different_dll_interface\n",
        "      dll_interface_conflict_none_assumed\n",
        "      dll_interface_conflict_dllexport_assumed\n",
        "      implicit_return_from_non_void_function\n",
        "      unsigned_compare_with_zero\n",
        "      declared_but_not_referenced\n",
        "      bad_friend_decl)\n",
        "      string(APPEND CMAKE_CUDA_FLAGS \" -Xcudafe --diag_suppress=${diag}\")\n",
        "    endforeach()\n",
        "    CUDA_CONVERT_FLAGS(torch_cpu)\n",
        "    if(TARGET torch_cuda)\n",
        "      CUDA_CONVERT_FLAGS(torch_cuda)\n",
        "    endif()\n",
        "    if(TARGET torch_cuda_cu)\n",
        "      CUDA_CONVERT_FLAGS(torch_cuda_cu)\n",
        "    endif()\n",
        "    if(TARGET torch_cuda_cpp)\n",
        "      CUDA_CONVERT_FLAGS(torch_cuda_cpp)\n",
        "    endif()\n",
        "  endif()\n",
        "endif()\n",
        "\n",
        "include(GNUInstallDirs)\n",
        "include(CMakePackageConfigHelpers)\n",
        "\n",
        "set(TVCPP torchvision/csrc)\n",
        "list(APPEND ALLOW_LISTED ${TVCPP} ${TVCPP}/ops\n",
        "  ${TVCPP}/ops/autograd ${TVCPP}/ops/cpu)\n",
        "if(WITH_CUDA)\n",
        "    list(APPEND ALLOW_LISTED ${TVCPP}/ops/cuda ${TVCPP}/ops/autocast)\n",
        "endif()\n",
        "\n",
        "FOREACH(DIR ${ALLOW_LISTED})\n",
        "    file(GLOB ALL_SOURCES ${ALL_SOURCES} ${DIR}/*.*)\n",
        "ENDFOREACH()\n",
        "\n",
        "add_library(${PROJECT_NAME} SHARED ${ALL_SOURCES})\n",
        "target_link_libraries(${PROJECT_NAME} PRIVATE ${TORCH_LIBRARIES})\n",
        "\n",
        "\n",
        "set_target_properties(${PROJECT_NAME} PROPERTIES\n",
        "  EXPORT_NAME TorchVision\n",
        "  INSTALL_RPATH ${TORCH_INSTALL_PREFIX}/lib)\n",
        "\n",
        "include_directories(torchvision/csrc ${JPEG_INCLUDE_DIRS} ${PNG_INCLUDE_DIRS})\n",
        "\n",
        "set(TORCHVISION_CMAKECONFIG_INSTALL_DIR \"share/cmake/TorchVision\" CACHE STRING \"install path for TorchVisionConfig.cmake\")\n",
        "\n",
        "configure_package_config_file(cmake/TorchVisionConfig.cmake.in\n",
        "  \"${CMAKE_CURRENT_BINARY_DIR}/TorchVisionConfig.cmake\"\n",
        "  INSTALL_DESTINATION ${TORCHVISION_CMAKECONFIG_INSTALL_DIR})\n",
        "\n",
        "write_basic_package_version_file(${CMAKE_CURRENT_BINARY_DIR}/TorchVisionConfigVersion.cmake\n",
        "  VERSION ${TORCHVISION_VERSION}\n",
        "  COMPATIBILITY AnyNewerVersion)\n",
        "\n",
        "install(FILES ${CMAKE_CURRENT_BINARY_DIR}/TorchVisionConfig.cmake\n",
        "  ${CMAKE_CURRENT_BINARY_DIR}/TorchVisionConfigVersion.cmake\n",
        "  DESTINATION ${TORCHVISION_CMAKECONFIG_INSTALL_DIR})\n",
        "\n",
        "install(TARGETS ${PROJECT_NAME}\n",
        "  EXPORT TorchVisionTargets\n",
        "  LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n",
        "  )\n",
        "\n",
        "install(EXPORT TorchVisionTargets\n",
        "  NAMESPACE TorchVision::\n",
        "  DESTINATION ${TORCHVISION_CMAKECONFIG_INSTALL_DIR})\n",
        "\n",
        "FOREACH(INPUT_DIR ${ALLOW_LISTED})\n",
        "    string(REPLACE \"${TVCPP}\" \"${CMAKE_INSTALL_INCLUDEDIR}/${PROJECT_NAME}\" OUTPUT_DIR ${INPUT_DIR})\n",
        "    file(GLOB INPUT_FILES ${INPUT_DIR}/*.*)\n",
        "    install(FILES ${INPUT_FILES} DESTINATION ${OUTPUT_DIR})\n",
        "ENDFOREACH()\n"
      ],
      "metadata": {
        "id": "4u-X-4o7s7_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cmake/TorchVisionConfig.cmake.in\n",
        "\n",
        "# TorchVisionConfig.cmake\n",
        "# --------------------\n",
        "#\n",
        "# Exported targets:: Vision\n",
        "#\n",
        "\n",
        "@PACKAGE_INIT@\n",
        "\n",
        "set(PN TorchVision)\n",
        "\n",
        "# location of include/torchvision\n",
        "set(${PN}_INCLUDE_DIR \"${PACKAGE_PREFIX_DIR}/@CMAKE_INSTALL_INCLUDEDIR@\")\n",
        "\n",
        "set(${PN}_LIBRARY \"\")\n",
        "set(${PN}_DEFINITIONS USING_${PN})\n",
        "\n",
        "check_required_components(${PN})\n",
        "\n",
        "\n",
        "if(NOT (CMAKE_VERSION VERSION_LESS 3.0))\n",
        "#-----------------------------------------------------------------------------\n",
        "# Don't include targets if this file is being picked up by another\n",
        "# project which has already built this as a subproject\n",
        "#-----------------------------------------------------------------------------\n",
        "if(NOT TARGET ${PN}::TorchVision)\n",
        "include(\"${CMAKE_CURRENT_LIST_DIR}/${PN}Targets.cmake\")\n",
        "\n",
        "if(NOT TARGET torch_library)\n",
        "find_package(Torch REQUIRED)\n",
        "endif()\n",
        "\n",
        "\n",
        "set_target_properties(TorchVision::TorchVision PROPERTIES INTERFACE_INCLUDE_DIRECTORIES \"${${PN}_INCLUDE_DIR}\" INTERFACE_LINK_LIBRARIES \"torch\" )\n",
        "\n",
        "\n",
        "if(@WITH_CUDA@)\n",
        "  target_compile_definitions(TorchVision::TorchVision INTERFACE WITH_CUDA)\n",
        "endif()\n",
        "\n",
        "endif()\n",
        "endif()\n"
      ],
      "metadata": {
        "id": "udqCX7OPtBUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile torchvision/csrc/vision.cpp\n",
        "\n",
        "#include \"vision.h\"\n",
        "\n",
        "#include <torch/library.h>\n",
        "\n",
        "#ifdef WITH_CUDA\n",
        "#include <cuda.h>\n",
        "#endif\n",
        "#ifdef WITH_HIP\n",
        "#include <hip/hip_runtime.h>\n",
        "#endif\n",
        "\n",
        "// If we are in a Windows environment, we need to define\n",
        "// initialization functions for the _custom_ops extension.\n",
        "// For PyMODINIT_FUNC to work, we need to include Python.h\n",
        "#if !defined(MOBILE) && defined(_WIN32)\n",
        "\n",
        "#endif // !defined(MOBILE) && defined(_WIN32)\n",
        "\n",
        "namespace vision {\n",
        "int64_t cuda_version() {\n",
        "#ifdef WITH_CUDA\n",
        "  return CUDA_VERSION;\n",
        "#else\n",
        "  return -1;\n",
        "#endif\n",
        "}\n",
        "\n",
        "TORCH_LIBRARY_FRAGMENT(torchvision, m) {\n",
        "  m.def(\"_cuda_version\", &cuda_version);\n",
        "}\n",
        "} // namespace vision\n"
      ],
      "metadata": {
        "id": "K7_EGulztOlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "!patch -p1 -i /content/ops.patch\n",
        "!patch -p1 -i /content/nopy.patch\n",
        "'''"
      ],
      "metadata": {
        "id": "hFnfuCqpnIJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# build torchvis"
      ],
      "metadata": {
        "id": "FwK08O_4s3BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p build\n",
        "%cd build"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gthfn5Xfn8w3",
        "outputId": "47aaabbf-fe30-4629-b83a-19ba9ea8ef0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/vision-0.13.0/build\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake -DCMAKE_INSTALL_PREFIX=\"/content/libtorch\" -DTorch_DIR=\"/content/libtorch/share/cmake/Torch\" -DCMAKE_BUILD_TYPE=Release .."
      ],
      "metadata": {
        "id": "Tc_qizg6oamn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake --build . -j 2"
      ],
      "metadata": {
        "id": "n9pOqdRjosOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake --build . --target install"
      ],
      "metadata": {
        "id": "-QagruH7owNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ncnn"
      ],
      "metadata": {
        "id": "XrfqfLXAtsb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/Tencent/ncnn.git"
      ],
      "metadata": {
        "id": "2r5RnYADtwMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ncnn/tools/pnnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HORSYMVXt_j1",
        "outputId": "96972321-89cb-4743-f9f2-b8be7207631f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ncnn/tools/pnnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir build\n",
        "%cd build"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKVgh5IJuPIX",
        "outputId": "875fc272-5760-41a8-88b2-0769d3757116"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ncnn/tools/pnnx/build\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake -DCMAKE_INSTALL_PREFIX=install -DTorch_INSTALL_DIR=\"/content/libtorch\" -DTorchVision_INSTALL_DIR=\"/content/libtorch\" -DCMAKE_BUILD_TYPE=Release .."
      ],
      "metadata": {
        "id": "YycF9jjyuS-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake --build . -j 2\n",
        "!cmake --build . --target install"
      ],
      "metadata": {
        "id": "dZM6HSZ2uZwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!mkdir pnnx\n",
        "!cp -l ncnn/tools/pnnx/build/install/bin/pnnx /content/pnnx\n",
        "!cp -l libtorch/lib/*.so /content/pnnx\n",
        "!cp -l libtorch/lib/libgomp*.so* /content/pnnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3NvS9-pwir0",
        "outputId": "81999903-6224-437a-aa06-f8744eb7874e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!7z a pnn.7z /content/pnnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TijBNREQyl3e",
        "outputId": "53a015ae-77d8-4011-c2c0-2fa412e26427"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive:\n",
            "  0M Scan  /content/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 folder, 13 files, 500280448 bytes (478 MiB)\n",
            "\n",
            "Creating archive: pnn.7z\n",
            "\n",
            "Items to compress: 14\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  0% 1 + pnnx/libtorchvision.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  0% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  1% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  2% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  3% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  9% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 15% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 8 + pnnx/libtorch_cpu.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 9 + pnnx/libtorch_global_deps.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 10 + pnnx/libtorch_python.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 10 + pnnx/libtorch_python.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 10 + pnnx/libtorch_python.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 10 + pnnx/libtorch_python.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 11 + pnnx/libtorchbind_test.so\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 12 + pnnx/pnnx\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 13 + pnnx/pnnx\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b100% 13 + pnnx/pnnx\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Files read from disk: 13\n",
            "Archive size: 80021432 bytes (77 MiB)\n",
            "Everything is Ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Klanly/buildcv/releases/download/100.200/g_bb.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWynOrV7y2AO",
        "outputId": "dccf9bca-2b8e-4871-88eb-be3ecc428b8b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-25 05:21:07--  https://github.com/Klanly/buildcv/releases/download/100.200/g_bb.pt\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/441862765/f52e35f7-f934-4d4a-b923-e79ea729bfae?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220725%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220725T052107Z&X-Amz-Expires=300&X-Amz-Signature=024e35688a9516d24d0263ec97ae30e94a0abf5c2c26882824b4e4c27dc2a806&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=441862765&response-content-disposition=attachment%3B%20filename%3Dg_bb.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-07-25 05:21:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/441862765/f52e35f7-f934-4d4a-b923-e79ea729bfae?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220725%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220725T052107Z&X-Amz-Expires=300&X-Amz-Signature=024e35688a9516d24d0263ec97ae30e94a0abf5c2c26882824b4e4c27dc2a806&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=441862765&response-content-disposition=attachment%3B%20filename%3Dg_bb.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 133053233 (127M) [application/octet-stream]\n",
            "Saving to: ‘g_bb.pt’\n",
            "\n",
            "g_bb.pt             100%[===================>] 126.89M  40.2MB/s    in 3.2s    \n",
            "\n",
            "2022-07-25 05:21:10 (40.2 MB/s) - ‘g_bb.pt’ saved [133053233/133053233]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/pnnx/pnnx /content/g_bb.pt inputshape=[1,1,512]f32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBdv7YoKzQC-",
        "outputId": "81843883-f165-43d3-bdc6-bfe0d3f14812"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pnnxparam = /content/g_bb.pnnx.param\n",
            "pnnxbin = /content/g_bb.pnnx.bin\n",
            "pnnxpy = /content/g_bb_pnnx.py\n",
            "ncnnparam = /content/g_bb.ncnn.param\n",
            "ncnnbin = /content/g_bb.ncnn.bin\n",
            "ncnnpy = /content/g_bb_ncnn.py\n",
            "optlevel = 2\n",
            "device = cpu\n",
            "inputshape = [1,1,512]f32\n",
            "inputshape2 = \n",
            "customop = \n",
            "moduleop = \n",
            "############# pass_level0\n",
            "inline module = model.Blur\n",
            "inline module = model.ConstantInput\n",
            "inline module = model.EqualLinear\n",
            "inline module = model.ModulatedConv2d\n",
            "inline module = model.NoiseInjection\n",
            "inline module = model.PixelNorm\n",
            "inline module = model.StyledConv\n",
            "inline module = model.ToRGB\n",
            "inline module = model.Upsample\n",
            "inline module = op.fused_act.FusedLeakyReLU\n",
            "inline module = model.Blur\n",
            "inline module = model.ConstantInput\n",
            "inline module = model.EqualLinear\n",
            "inline module = model.ModulatedConv2d\n",
            "inline module = model.NoiseInjection\n",
            "inline module = model.PixelNorm\n",
            "inline module = model.StyledConv\n",
            "inline module = model.ToRGB\n",
            "inline module = model.Upsample\n",
            "inline module = op.fused_act.FusedLeakyReLU\n",
            "input0.3  400  405  407  408  409  410  411  412  414  bias.7  weight.5  420  inputs.7  bias0.2  424  427  input.5  429  430  431  432  434  bias.4  weight.4  440  inputs.4  bias0.4  444  447  input.7  449  450  451  452  454  bias.6  weight.6  460  inputs.6  bias0.6  464  467  input.9  469  470  471  472  474  bias.8  weight.8  480  inputs.8  bias0.8  484  487  input.11  489  490  491  492  494  bias.10  weight.10  500  inputs.10  bias0.10  504  507  input.13  509  510  511  512  514  bias.12  weight.12  520  inputs.12  bias0.12  524  527  input.15  529  530  531  532  534  bias.14  weight.14  540  inputs.14  bias0.14  544  547  input.17  549  550  551  552  554  bias.5  weight.7  560  inputs.15  bias0.1  564  567  input.8  569  570  114  input1.55  input.3  batch.3  input1.1  126  input2.1  583  584  587  weight.9  batch.7  in_channel.2  height.5  width.5  611  612  bias.11  weight.15  615  616  617  style.5  620  weight0.2  622  624  625  demod.2  628  weight1.2  630  weight2.2  634  input0.5  out.2  height0.2  width0.2  image.2  weight.11  batch.5  height.3  width.3  672  noise.6  674  inputs.5  676  bias.9  682  685  input.10  687  688  135  input3.1  bias.19  695  696  weight.13  batch.9  in_channel.4  height.7  width.7  717  718  bias.13  weight.17  721  722  723  style.7  726  weight0.4  728  weight1.4  732  input0.7  out.4  height0.4  width0.4  out0.4  inputs.9  144  input4.1  754  755  758  weight.19  batch.11  in_channel.6  height.9  width.9  785  786  bias.15  weight.21  789  790  791  style.9  794  weight0.6  796  798  799  demod.4  802  weight1.6  804  weight2.4  808  input0.9  weight3.2  814  815  weight4.2  out.10  height0.6  width0.6  inputs.17  835  836  kernel.2  channel.2  in_h.2  in_w.2  inputs0.2  in_h0.2  in_w0.2  minor.2  kernel_h.2  kernel_w.2  out.6  out0.6  878  5210  880  out1.2  out2.2  887  888  891  5223  892  894  895  896  out3.2  out4.2  5242  900  5243  901  5245  902  5247  904  5248  905  5250  906  out5.2  911  w.2  out6.2  5271  919  5272  920  5274  921  922  5277  923  5279  925  5280  926  5282  927  928  5285  929  out7.2  out8.2  935  936  937  out9.2  5308  939  5309  940  5311  941  942  5314  943  5316  out_h.2  5317  946  5318  947  5320  948  949  5323  950  5325  out_w.2  image.4  weight.23  batch.13  height.11  width.11  976  noise.10  978  inputs.11  980  bias.17  986  989  input.12  991  992  153  input5.1  997  998  1001  weight.25  batch.17  in_channel.8  height.15  width.15  1025  1026  bias.23  weight.31  1029  1030  1031  style.11  1034  weight0.8  1036  1038  1039  demod.6  1042  weight1.8  1044  weight2.6  1048  input0.11  out.8  height0.8  width0.8  image.6  weight.27  batch.15  height.13  width.13  1086  noise.14  1088  inputs.13  1090  bias.21  1096  1099  input.14  1101  1102  163  input6.1  bias.31  1110  1111  weight.29  batch.19  in_channel.10  height.17  width.17  1132  1133  bias.25  weight.33  1136  1137  1138  style.13  1141  weight0.10  1143  weight1.10  1147  input0.13  out.16  height0.10  width0.10  out0.10  out.18  1168  1169  1170  kernel.4  channel.4  in_h.4  in_w.4  inputs.19  in_h0.4  in_w0.4  minor.4  kernel_h.4  kernel_w.4  out.12  out0.8  1212  5415  1214  out1.4  out2.4  1221  1222  1225  5428  1226  1228  1229  1230  out3.4  out4.4  5447  1234  5448  1235  1236  5451  1238  5452  1239  5454  1240  out5.4  1245  w.4  out6.4  5475  1253  5476  1254  5478  1255  1256  5481  1257  5483  1259  5484  1260  5486  1261  1262  5489  1263  out7.4  out8.4  1269  1270  1271  out9.4  5512  1273  5513  1274  5515  1275  1276  5518  1277  5520  out_h.4  5521  1280  5522  1281  5524  1282  1283  5527  1284  5529  out_w.4  skip.2  inputs.25  174  input7.1  1294  1295  1298  weight.35  batch.21  in_channel.12  height.19  width.19  1325  1326  bias.27  weight.37  1329  1330  1331  style.15  1334  weight0.12  1336  1338  1339  demod.8  1342  weight1.12  1344  weight2.8  1348  input0.15  weight3.4  1354  1355  weight4.4  out.22  height0.12  width0.12  inputs.27  1375  1376  kernel.6  channel.6  in_h.6  in_w.6  inputs0.4  in_h0.6  in_w0.6  minor.6  kernel_h.6  kernel_w.6  out.14  out0.12  1418  5593  1420  out1.6  out2.6  1427  1428  1431  5606  1432  1434  1435  1436  out3.6  out4.6  5625  1440  5626  1441  5628  1442  5630  1444  5631  1445  5633  1446  out5.6  1451  w.6  out6.6  5654  1459  5655  1460  5657  1461  1462  5660  1463  5662  1465  5663  1466  5665  1467  1468  5668  1469  out7.6  out8.6  1475  1476  1477  out9.6  5691  1479  5692  1480  5694  1481  1482  5697  1483  5699  out_h.6  5700  1486  5701  1487  5703  1488  1489  5706  1490  5708  out_w.6  image.8  weight.39  batch.23  height.21  width.21  1516  noise.18  1518  inputs.21  1520  bias.29  1526  1529  input.16  1531  1532  183  input8.1  1537  1538  1541  weight.41  batch.27  in_channel.14  height.25  width.25  1565  1566  bias.35  weight.47  1569  1570  1571  style.17  1574  weight0.14  1576  1578  1579  demod.10  1582  weight1.14  1584  weight2.10  1588  input0.17  out.20  height0.14  width0.14  image.10  weight.43  batch.25  height.23  width.23  1626  noise.22  1628  inputs.23  1630  bias.33  1636  1639  input.18  1641  1642  193  input9.1  bias.43  1650  1651  weight.45  batch.29  in_channel.16  height.27  width.27  1672  1673  bias.37  weight.49  1676  1677  1678  style.19  1681  weight0.16  1683  weight1.16  1687  input0.19  out.28  height0.16  width0.16  out0.16  out.30  1708  1709  1710  kernel.8  channel.8  in_h.8  in_w.8  inputs.29  in_h0.8  in_w0.8  minor.8  kernel_h.8  kernel_w.8  out.24  out0.14  1752  5798  1754  out1.8  out2.8  1761  1762  1765  5811  1766  1768  1769  1770  out3.8  out4.8  5830  1774  5831  1775  1776  5834  1778  5835  1779  5837  1780  out5.8  1785  w.8  out6.8  5858  1793  5859  1794  5861  1795  1796  5864  1797  5866  1799  5867  1800  5869  1801  1802  5872  1803  out7.8  out8.8  1809  1810  1811  out9.8  5895  1813  5896  1814  5898  1815  1816  5901  1817  5903  out_h.8  5904  1820  5905  1821  5907  1822  1823  5910  1824  5912  out_w.8  skip.4  inputs.35  204  input10.1  1834  1835  1838  weight.51  batch.31  in_channel.18  height.29  width.29  1865  1866  bias.39  weight.53  1869  1870  1871  style.21  1874  weight0.18  1876  1878  1879  demod.12  1882  weight1.18  1884  weight2.12  1888  input0.21  weight3.6  1894  1895  weight4.6  out.34  height0.18  width0.18  inputs.37  1915  1916  kernel.10  channel.10  in_h.10  in_w.10  inputs0.6  in_h0.10  in_w0.10  minor.10  kernel_h.10  kernel_w.10  out.26  out0.18  1958  5976  1960  out1.10  out2.10  1967  1968  1971  5989  1972  1974  1975  1976  out3.10  out4.10  6008  1980  6009  1981  6011  1982  6013  1984  6014  1985  6016  1986  out5.10  1991  w.10  out6.10  6037  1999  6038  2000  6040  2001  2002  6043  2003  6045  2005  6046  2006  6048  2007  2008  6051  2009  out7.10  out8.10  2015  2016  2017  out9.10  6074  2019  6075  2020  6077  2021  2022  6080  2023  6082  out_h.10  6083  2026  6084  2027  6086  2028  2029  6089  2030  6091  out_w.10  image.12  weight.55  batch.33  height.31  width.31  2056  noise.26  2058  inputs.31  2060  bias.41  2066  2069  input.20  2071  2072  213  input11.1  2077  2078  2081  weight.57  batch.37  in_channel.20  height.35  width.35  2105  2106  bias.47  weight.63  2109  2110  2111  style.23  2114  weight0.20  2116  2118  2119  demod.14  2122  weight1.20  2124  weight2.14  2128  \n",
            "----------------\n",
            "\n",
            "inline module = model.Blur\n",
            "inline module = model.ConstantInput\n",
            "inline module = model.EqualLinear\n",
            "inline module = model.ModulatedConv2d\n",
            "inline module = model.NoiseInjection\n",
            "inline module = model.PixelNorm\n",
            "inline module = model.StyledConv\n",
            "inline module = model.ToRGB\n",
            "inline module = model.Upsample\n",
            "inline module = op.fused_act.FusedLeakyReLU\n",
            "input0.23  out.32  height0.20  width0.20  image.14  weight.59  batch.35  height.33  width.33  2166  noise.30  2168  inputs.33  2170  bias.45  2176  2179  input.22  2181  2182  223  input12.1  bias.55  2190  2191  weight.61  batch.39  in_channel.22  height.37  width.37  2212  2213  bias.49  weight.65  2216  2217  2218  style.25  2221  weight0.22  2223  weight1.22  2227  input0.25  out.40  height0.22  width0.22  out0.22  out.42  2248  2249  2250  kernel.12  channel.12  in_h.12  in_w.12  inputs.39  in_h0.12  in_w0.12  minor.12  kernel_h.12  kernel_w.12  out.36  out0.20  2292  6181  2294  out1.12  out2.12  2301  2302  2305  6194  2306  2308  2309  2310  out3.12  out4.12  6213  2314  6214  2315  2316  6217  2318  6218  2319  6220  2320  out5.12  2325  w.12  out6.12  6241  2333  6242  2334  6244  2335  2336  6247  2337  6249  2339  6250  2340  6252  2341  2342  6255  2343  out7.12  out8.12  2349  2350  2351  out9.12  6278  2353  6279  2354  6281  2355  2356  6284  2357  6286  out_h.12  6287  2360  6288  2361  6290  2362  2363  6293  2364  6295  out_w.12  skip.6  inputs.45  234  input13.1  2374  2375  2378  weight.67  batch.41  in_channel.24  height.39  width.39  2405  2406  bias.51  weight.69  2409  2410  2411  style.27  2414  weight0.24  2416  2418  2419  demod.16  2422  weight1.24  2424  weight2.16  2428  input0.27  weight3.8  2434  2435  weight4.8  out.46  height0.24  width0.24  inputs.47  2455  2456  kernel.14  channel.14  in_h.14  in_w.14  inputs0.8  in_h0.14  in_w0.14  minor.14  kernel_h.14  kernel_w.14  out.38  out0.24  2498  6359  2500  out1.14  out2.14  2507  2508  2511  6372  2512  2514  2515  2516  out3.14  out4.14  6391  2520  6392  2521  6394  2522  6396  2524  6397  2525  6399  2526  out5.14  2531  w.14  out6.14  6420  2539  6421  2540  6423  2541  2542  6426  2543  6428  2545  6429  2546  6431  2547  2548  6434  2549  out7.14  out8.14  2555  2556  2557  out9.14  6457  2559  6458  2560  6460  2561  2562  6463  2563  6465  out_h.14  6466  2566  6467  2567  6469  2568  2569  6472  2570  6474  out_w.14  image.16  weight.71  batch.43  height.41  width.41  2596  noise.34  2598  inputs.41  2600  bias.53  2606  2609  input.24  2611  2612  243  input14.1  2617  2618  2621  weight.73  batch.47  in_channel.26  height.45  width.45  2645  2646  bias.59  weight.79  2649  2650  2651  style.29  2654  weight0.26  2656  2658  2659  demod.18  2662  weight1.26  2664  weight2.18  2668  input0.29  out.44  height0.26  width0.26  image.18  weight.75  batch.45  height.43  width.43  2706  noise.38  2708  inputs.43  2710  bias.57  2716  2719  input.26  2721  2722  253  input15.1  bias.67  2730  2731  weight.77  batch.49  in_channel.28  height.47  width.47  2752  2753  bias.61  weight.81  2756  2757  2758  style.31  2761  weight0.28  2763  weight1.28  2767  input0.31  out.52  height0.28  width0.28  out0.28  out.54  2788  2789  2790  kernel.16  channel.16  in_h.16  in_w.16  inputs.49  in_h0.16  in_w0.16  minor.16  kernel_h.16  kernel_w.16  out.48  out0.26  2832  6564  2834  out1.16  out2.16  2841  2842  2845  6577  2846  2848  2849  2850  out3.16  out4.16  6596  2854  6597  2855  2856  6600  2858  6601  2859  6603  2860  out5.16  2865  w.16  out6.16  6624  2873  6625  2874  6627  2875  2876  6630  2877  6632  2879  6633  2880  6635  2881  2882  6638  2883  out7.16  out8.16  2889  2890  2891  out9.16  6661  2893  6662  2894  6664  2895  2896  6667  2897  6669  out_h.16  6670  2900  6671  2901  6673  2902  2903  6676  2904  6678  out_w.16  skip.8  inputs.55  264  input16.1  2914  2915  2918  weight.83  batch.51  in_channel.30  height.49  width.49  2945  2946  bias.63  weight.85  2949  2950  2951  style.33  2954  weight0.30  2956  2958  2959  demod.20  2962  weight1.30  2964  weight2.20  2968  input0.33  weight3.10  2974  2975  weight4.10  out.58  height0.30  width0.30  inputs.57  2995  2996  kernel.18  channel.18  in_h.18  in_w.18  inputs0.10  in_h0.18  in_w0.18  minor.18  kernel_h.18  kernel_w.18  out.50  out0.30  3038  6742  3040  out1.18  out2.18  3047  3048  3051  6755  3052  3054  3055  3056  out3.18  out4.18  6774  3060  6775  3061  6777  3062  6779  3064  6780  3065  6782  3066  out5.18  3071  w.18  out6.18  6803  3079  6804  3080  6806  3081  3082  6809  3083  6811  3085  6812  3086  6814  3087  3088  6817  3089  out7.18  out8.18  3095  3096  3097  out9.18  6840  3099  6841  3100  6843  3101  3102  6846  3103  6848  out_h.18  6849  3106  6850  3107  6852  3108  3109  6855  3110  6857  out_w.18  image.20  weight.87  batch.53  height.51  width.51  3136  noise.42  3138  inputs.51  3140  bias.65  3146  3149  input.28  3151  3152  273  input17.1  3157  3158  3161  weight.89  batch.57  in_channel.32  height.55  width.55  3185  3186  bias.71  weight.95  3189  3190  3191  style.35  3194  weight0.32  3196  3198  3199  demod.22  3202  weight1.32  3204  weight2.22  3208  input0.35  out.56  height0.32  width0.32  image.22  weight.91  batch.55  height.53  width.53  3246  noise.46  3248  inputs.53  3250  bias.69  3256  3259  input.30  3261  3262  283  input18.1  bias.79  3270  3271  weight.93  batch.59  in_channel.34  height.57  width.57  3292  3293  bias.73  weight.97  3296  3297  3298  style.37  3301  weight0.34  3303  weight1.34  3307  input0.37  out.64  height0.34  width0.34  out0.34  out.66  3328  3329  3330  kernel.20  channel.20  in_h.20  in_w.20  inputs.59  in_h0.20  in_w0.20  minor.20  kernel_h.20  kernel_w.20  out.60  out0.32  3372  6947  3374  out1.20  out2.20  3381  3382  3385  6960  3386  3388  3389  3390  out3.20  out4.20  6979  3394  6980  3395  3396  6983  3398  6984  3399  6986  3400  out5.20  3405  w.20  out6.20  7007  3413  7008  3414  7010  3415  3416  7013  3417  7015  3419  7016  3420  7018  3421  3422  7021  3423  out7.20  out8.20  3429  3430  3431  out9.20  7044  3433  7045  3434  7047  3435  3436  7050  3437  7052  out_h.20  7053  3440  7054  3441  7056  3442  3443  7059  3444  7061  out_w.20  skip.10  inputs.65  294  input19.1  3454  3455  3458  weight.99  batch.61  in_channel.36  height.59  width.59  3485  3486  bias.75  weight.101  3489  3490  3491  style.39  3494  weight0.36  3496  3498  3499  demod.24  3502  weight1.36  3504  weight2.24  3508  input0.39  weight3.12  3514  3515  weight4.12  out.70  height0.36  width0.36  inputs.67  3535  3536  kernel.22  channel.22  in_h.22  in_w.22  inputs0.12  in_h0.22  in_w0.22  minor.22  kernel_h.22  kernel_w.22  out.62  out0.36  3578  7125  3580  out1.22  out2.22  3587  3588  3591  7138  3592  3594  3595  3596  out3.22  out4.22  7157  3600  7158  3601  7160  3602  7162  3604  7163  3605  7165  3606  out5.22  3611  w.22  out6.22  7186  3619  7187  3620  7189  3621  3622  7192  3623  7194  3625  7195  3626  7197  3627  3628  7200  3629  out7.22  out8.22  3635  3636  3637  out9.22  7223  3639  7224  3640  7226  3641  3642  7229  3643  7231  out_h.22  7232  3646  7233  3647  7235  3648  3649  7238  3650  7240  out_w.22  image.24  weight.103  batch.63  height.61  width.61  3676  noise.50  3678  inputs.61  3680  bias.77  3686  3689  input.32  3691  3692  303  input20.1  3697  3698  3701  weight.105  batch.67  in_channel.38  height.65  width.65  3725  3726  bias.83  weight.111  3729  3730  3731  style.41  3734  weight0.38  3736  3738  3739  demod.26  3742  weight1.38  3744  weight2.26  3748  input0.41  out.68  height0.38  width0.38  image.26  weight.107  batch.65  height.63  width.63  3786  noise.54  3788  inputs.63  3790  bias.81  3796  3799  input.34  3801  3802  313  input21.1  bias.91  3810  3811  weight.109  batch.69  in_channel.40  height.67  width.67  3832  3833  bias.85  weight.113  3836  3837  3838  style.43  3841  weight0.40  3843  weight1.40  3847  input0.43  out.76  height0.40  width0.40  out0.40  out.78  3868  3869  3870  \n",
            "----------------\n",
            "\n",
            "inline module = model.Blur\n",
            "inline module = model.ConstantInput\n",
            "inline module = model.EqualLinear\n",
            "inline module = model.ModulatedConv2d\n",
            "inline module = model.NoiseInjection\n",
            "inline module = model.PixelNorm\n",
            "inline module = model.StyledConv\n",
            "inline module = model.ToRGB\n",
            "inline module = model.Upsample\n",
            "inline module = op.fused_act.FusedLeakyReLU\n",
            "kernel.24  channel.24  in_h.24  in_w.24  inputs.69  in_h0.24  in_w0.24  minor.24  kernel_h.24  kernel_w.24  out.72  out0.38  3912  7330  3914  out1.24  out2.24  3921  3922  3925  7343  3926  3928  3929  3930  out3.24  out4.24  7362  3934  7363  3935  3936  7366  3938  7367  3939  7369  3940  out5.24  3945  w.24  out6.24  7390  3953  7391  3954  7393  3955  3956  7396  3957  7398  3959  7399  3960  7401  3961  3962  7404  3963  out7.24  out8.24  3969  3970  3971  out9.24  7427  3973  7428  3974  7430  3975  3976  7433  3977  7435  out_h.24  7436  3980  7437  3981  7439  3982  3983  7442  3984  7444  out_w.24  skip.12  inputs.75  324  input22.1  3994  3995  3998  weight.115  batch.71  in_channel.42  height.69  width.69  4025  4026  bias.87  weight.117  4029  4030  4031  style.45  4034  weight0.42  4036  4038  4039  demod.28  4042  weight1.42  4044  weight2.28  4048  input0.45  weight3.14  4054  4055  weight4.14  out.82  height0.42  width0.42  inputs.77  4075  4076  kernel.26  channel.26  in_h.26  in_w.26  inputs0.14  in_h0.26  in_w0.26  minor.26  kernel_h.26  kernel_w.26  out.74  out0.42  4118  7508  4120  out1.26  out2.26  4127  4128  4131  7521  4132  4134  4135  4136  out3.26  out4.26  7540  4140  7541  4141  7543  4142  7545  4144  7546  4145  7548  4146  out5.26  4151  w.26  out6.26  7569  4159  7570  4160  7572  4161  4162  7575  4163  7577  4165  7578  4166  7580  4167  4168  7583  4169  out7.26  out8.26  4175  4176  4177  out9.26  7606  4179  7607  4180  7609  4181  4182  7612  4183  7614  out_h.26  7615  4186  7616  4187  7618  4188  4189  7621  4190  7623  out_w.26  image.28  weight.119  batch.73  height.71  width.71  4216  noise.58  4218  inputs.71  4220  bias.89  4226  4229  input.36  4231  4232  333  input23.1  4237  4238  4241  weight.121  batch.77  in_channel.44  height.75  width.75  4265  4266  bias.95  weight.127  4269  4270  4271  style.47  4274  weight0.44  4276  4278  4279  demod.30  4282  weight1.44  4284  weight2.30  4288  input0.47  out.80  height0.44  width0.44  image.30  weight.123  batch.75  height.73  width.73  4326  noise.62  4328  inputs.73  4330  bias.93  4336  4339  input.38  4341  4342  343  input24.1  bias.103  4350  4351  weight.125  batch.79  in_channel.46  height.77  width.77  4372  4373  bias.97  weight.129  4376  4377  4378  style.49  4381  weight0.46  4383  weight1.46  4387  input0.49  out.88  height0.46  width0.46  out0.46  out.90  4408  4409  4410  kernel.28  channel.28  in_h.28  in_w.28  inputs.79  in_h0.28  in_w0.28  minor.28  kernel_h.28  kernel_w.28  out.84  out0.44  4452  7713  4454  out1.28  out2.28  4461  4462  4465  7726  4466  4468  4469  4470  out3.28  out4.28  7745  4474  7746  4475  4476  7749  4478  7750  4479  7752  4480  out5.28  4485  w.28  out6.28  7773  4493  7774  4494  7776  4495  4496  7779  4497  7781  4499  7782  4500  7784  4501  4502  7787  4503  out7.28  out8.28  4509  4510  4511  out9.28  7810  4513  7811  4514  7813  4515  4516  7816  4517  7818  out_h.28  7819  4520  7820  4521  7822  4522  4523  7825  4524  7827  out_w.28  skip.14  inputs.3  354  input25.1  4534  4535  4538  weight.131  batch.81  in_channel.48  height.79  width.79  4565  4566  bias.99  weight.133  4569  4570  4571  style.51  4574  weight0.48  4576  4578  4579  demod.32  4582  weight1.48  4584  weight2.32  4588  input0.51  weight3.1  4594  4595  weight4.1  out.7  height0.48  width0.48  inputs.2  4615  4616  kernel.30  channel.30  in_h.30  in_w.30  inputs0.1  in_h0.30  in_w0.30  minor.30  kernel_h.30  kernel_w.30  out.86  out0.48  4658  7891  4660  out1.30  out2.30  4667  4668  4671  7904  4672  4674  4675  4676  out3.30  out4.30  7923  4680  7924  4681  7926  4682  7928  4684  7929  4685  7931  4686  out5.30  4691  w.30  out6.30  7952  4699  7953  4700  7955  4701  4702  7958  4703  7960  4705  7961  4706  7963  4707  4708  7966  4709  out7.30  out8.30  4715  4716  4717  out9.30  7989  4719  7990  4720  7992  4721  4722  7995  4723  7997  out_h.30  7998  4726  7999  4727  8001  4728  4729  8004  4730  8006  out_w.30  image.32  weight.135  batch.83  height.81  width.81  4756  noise.66  4758  inputs.81  4760  bias.101  4766  4769  input.40  4771  4772  363  input26.1  4777  4778  4781  weight.137  batch.2  in_channel.50  height.2  width.2  4805  4806  bias.2  weight.2  4809  4810  4811  style.53  4814  weight0.50  4816  4818  4819  demod.1  4822  weight1.50  4824  weight2.1  4828  input0.53  out.92  height0.50  width0.50  image.1  weight.139  batch.85  height.83  width.83  4866  noise.1  4868  inputs.83  4870  bias.105  4876  4879  input.1  4881  4882  373  input27.1  bias.3  4890  4891  weight.3  batch.1  in_channel.1  height.1  width.1  4912  4913  bias.1  weight.1  4916  4917  4918  style.1  4921  weight0.1  4923  weight1.1  4927  input0.1  out.5  height0.1  width0.1  out0.2  out.3  4948  4949  4950  kernel.1  channel.1  in_h.1  in_w.1  inputs.1  in_h0.1  in_w0.1  minor.1  kernel_h.1  kernel_w.1  out.1  out0.1  4992  8096  4994  out1.1  out2.1  5001  5002  5005  8109  5006  5008  5009  5010  out3.1  out4.1  8128  5014  8129  5015  5016  8132  5018  8133  5019  8135  5020  out5.1  5025  w.1  out6.1  8156  5033  8157  5034  8159  5035  5036  8162  5037  8164  5039  8165  5040  8167  5041  5042  8170  5043  out7.1  out8.1  5049  5050  5051  out9.1  8193  5053  8194  5054  8196  5055  5056  8199  5057  8201  out_h.1  8202  5060  8203  5061  8205  5062  5063  8208  5064  8210  out_w.1  skip.1  5069  \n",
            "----------------\n",
            "\n",
            "tcmalloc: large alloc 1073741824 bytes == 0x55c7c7436000 @  0x7f9e30946b6b 0x7f9e30966379 0x7f9e30d6863a 0x7f9e30d4de62 0x7f9e197b2b38 0x7f9e1912a09c 0x7f9e196de2c7 0x7f9e196e19b3 0x7f9e197ab7f5 0x7f9e19ee6612 0x7f9e198b16eb 0x7f9e191f0e14 0x7f9e19ee575d 0x7f9e19872780 0x7f9e1ad6bd18 0x7f9e1adde17b 0x7f9e18f46c69 0x7f9e1ba5fbd3 0x7f9e1ba4df62 0x7f9e1ba41086 0x7f9e1b6d42d0 0x55c7294d9c9b 0x55c7294d723d 0x55c72949f365 0x7f9e17091c87 0x55c7294aa8fa\n",
            "foldable_constant bias0.8\n",
            "foldable_constant 480\n",
            "foldable_constant 460\n",
            "foldable_constant bias0.6\n",
            "foldable_constant 467\n",
            "foldable_constant 487\n",
            "foldable_constant 420\n",
            "foldable_constant bias0.2\n",
            "foldable_constant 427\n",
            "foldable_constant 440\n",
            "foldable_constant bias0.4\n",
            "foldable_constant 447\n",
            "foldable_constant 500\n",
            "foldable_constant bias0.10\n",
            "foldable_constant 507\n",
            "foldable_constant 520\n",
            "foldable_constant bias0.12\n",
            "foldable_constant 527\n",
            "foldable_constant 540\n",
            "foldable_constant bias0.14\n",
            "foldable_constant 547\n",
            "foldable_constant 560\n",
            "foldable_constant bias0.1\n",
            "foldable_constant 567\n",
            "foldable_constant input1.1\n",
            "foldable_constant 615\n",
            "foldable_constant 616\n",
            "foldable_constant 620\n",
            "foldable_constant input0.5\n",
            "foldable_constant 674\n",
            "foldable_constant bias.9\n",
            "foldable_constant 685\n",
            "foldable_constant bias.19\n",
            "foldable_constant 721\n",
            "foldable_constant 722\n",
            "foldable_constant 726\n",
            "foldable_constant 789\n",
            "foldable_constant 790\n",
            "foldable_constant 794\n",
            "foldable_constant kernel.2\n",
            "foldable_constant w.2\n",
            "foldable_constant 978\n",
            "foldable_constant bias.17\n",
            "foldable_constant 989\n",
            "foldable_constant 1029\n",
            "foldable_constant 1030\n",
            "foldable_constant 1034\n",
            "foldable_constant 1088\n",
            "foldable_constant bias.21\n",
            "foldable_constant 1099\n",
            "foldable_constant bias.31\n",
            "foldable_constant 1136\n",
            "foldable_constant 1137\n",
            "foldable_constant 1141\n",
            "foldable_constant kernel.4\n",
            "foldable_constant w.4\n",
            "foldable_constant 1329\n",
            "foldable_constant 1330\n",
            "foldable_constant 1334\n",
            "foldable_constant kernel.6\n",
            "foldable_constant w.6\n",
            "foldable_constant 1518\n",
            "foldable_constant bias.29\n",
            "foldable_constant 1529\n",
            "foldable_constant 1569\n",
            "foldable_constant 1570\n",
            "foldable_constant 1574\n",
            "foldable_constant 1628\n",
            "foldable_constant bias.33\n",
            "foldable_constant 1639\n",
            "foldable_constant bias.43\n",
            "foldable_constant 1676\n",
            "foldable_constant 1677\n",
            "foldable_constant 1681\n",
            "foldable_constant kernel.8\n",
            "foldable_constant w.8\n",
            "foldable_constant 1869\n",
            "foldable_constant 1870\n",
            "foldable_constant 1874\n",
            "foldable_constant kernel.10\n",
            "foldable_constant w.10\n",
            "foldable_constant 2058\n",
            "foldable_constant bias.41\n",
            "foldable_constant 2069\n",
            "foldable_constant 2109\n",
            "foldable_constant 2110\n",
            "foldable_constant 2114\n",
            "foldable_constant 2168\n",
            "foldable_constant bias.45\n",
            "foldable_constant 2179\n",
            "foldable_constant bias.55\n",
            "foldable_constant 2216\n",
            "foldable_constant 2217\n",
            "foldable_constant 2221\n",
            "foldable_constant kernel.12\n",
            "foldable_constant w.12\n",
            "foldable_constant 2409\n",
            "foldable_constant 2410\n",
            "foldable_constant 2414\n",
            "foldable_constant kernel.14\n",
            "foldable_constant w.14\n",
            "foldable_constant 2598\n",
            "foldable_constant bias.53\n",
            "foldable_constant 2609\n",
            "foldable_constant 2649\n",
            "foldable_constant 2650\n",
            "foldable_constant 2654\n",
            "foldable_constant 2708\n",
            "foldable_constant bias.57\n",
            "foldable_constant 2719\n",
            "foldable_constant bias.67\n",
            "foldable_constant 2756\n",
            "foldable_constant 2757\n",
            "foldable_constant 2761\n",
            "foldable_constant kernel.16\n",
            "foldable_constant w.16\n",
            "foldable_constant 2949\n",
            "foldable_constant 2950\n",
            "foldable_constant 2954\n",
            "foldable_constant kernel.18\n",
            "foldable_constant w.18\n",
            "foldable_constant 3138\n",
            "foldable_constant bias.65\n",
            "foldable_constant 3149\n",
            "foldable_constant 3189\n",
            "foldable_constant 3190\n",
            "foldable_constant 3194\n",
            "foldable_constant 3248\n",
            "foldable_constant bias.69\n",
            "foldable_constant 3259\n",
            "foldable_constant bias.79\n",
            "foldable_constant 3296\n",
            "foldable_constant 3297\n",
            "foldable_constant 3301\n",
            "foldable_constant kernel.20\n",
            "foldable_constant w.20\n",
            "foldable_constant 3489\n",
            "foldable_constant 3490\n",
            "foldable_constant 3494\n",
            "foldable_constant kernel.22\n",
            "foldable_constant w.22\n",
            "foldable_constant 3678\n",
            "foldable_constant bias.77\n",
            "foldable_constant 3689\n",
            "foldable_constant 3729\n",
            "foldable_constant 3730\n",
            "foldable_constant 3734\n",
            "foldable_constant 3788\n",
            "foldable_constant bias.81\n",
            "foldable_constant 3799\n",
            "foldable_constant bias.91\n",
            "foldable_constant 3836\n",
            "foldable_constant 3837\n",
            "foldable_constant 3841\n",
            "foldable_constant kernel.24\n",
            "foldable_constant w.24\n",
            "foldable_constant 4029\n",
            "foldable_constant 4030\n",
            "foldable_constant 4034\n",
            "foldable_constant kernel.26\n",
            "foldable_constant w.26\n",
            "foldable_constant 4218\n",
            "foldable_constant bias.89\n",
            "foldable_constant 4229\n",
            "foldable_constant 4269\n",
            "foldable_constant 4270\n",
            "foldable_constant 4274\n",
            "foldable_constant 4328\n",
            "foldable_constant bias.93\n",
            "foldable_constant 4339\n",
            "foldable_constant bias.103\n",
            "foldable_constant 4376\n",
            "foldable_constant 4377\n",
            "foldable_constant 4381\n",
            "foldable_constant kernel.28\n",
            "foldable_constant w.28\n",
            "foldable_constant 4569\n",
            "foldable_constant 4570\n",
            "foldable_constant 4574\n",
            "foldable_constant kernel.30\n",
            "foldable_constant w.30\n",
            "foldable_constant 4758\n",
            "foldable_constant bias.101\n",
            "foldable_constant 4769\n",
            "foldable_constant 4809\n",
            "foldable_constant 4810\n",
            "foldable_constant 4814\n",
            "foldable_constant 4868\n",
            "foldable_constant bias.105\n",
            "foldable_constant 4879\n",
            "foldable_constant bias.3\n",
            "foldable_constant 4916\n",
            "foldable_constant 4917\n",
            "foldable_constant 4921\n",
            "foldable_constant kernel.1\n",
            "foldable_constant w.1\n",
            "############# pass_level1\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "unknown Parameter value kind prim::Constant\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "no attribute value\n",
            "############# pass_level2\n",
            "############# pass_level3\n",
            "assign unique operator name pnnx_unique_0 to style.1\n",
            "assign unique operator name pnnx_unique_1 to style.2\n",
            "assign unique operator name pnnx_unique_2 to style.3\n",
            "assign unique operator name pnnx_unique_3 to style.4\n",
            "assign unique operator name pnnx_unique_4 to style.5\n",
            "assign unique operator name pnnx_unique_5 to style.6\n",
            "assign unique operator name pnnx_unique_6 to style.7\n",
            "assign unique operator name pnnx_unique_7 to style.8\n",
            "assign unique operator name pnnx_unique_8 to conv1.conv.modulation\n",
            "assign unique operator name pnnx_unique_9 to to_rgb1.conv.modulation\n",
            "assign unique operator name pnnx_unique_10 to convs.0.conv.modulation\n",
            "assign unique operator name pnnx_unique_11 to convs.1.conv.modulation\n",
            "assign unique operator name pnnx_unique_12 to to_rgbs.0.conv.modulation\n",
            "assign unique operator name pnnx_unique_13 to convs.2.conv.modulation\n",
            "assign unique operator name pnnx_unique_14 to convs.3.conv.modulation\n",
            "assign unique operator name pnnx_unique_15 to to_rgbs.1.conv.modulation\n",
            "assign unique operator name pnnx_unique_16 to convs.4.conv.modulation\n",
            "assign unique operator name pnnx_unique_17 to convs.5.conv.modulation\n",
            "assign unique operator name pnnx_unique_18 to to_rgbs.2.conv.modulation\n",
            "assign unique operator name pnnx_unique_19 to convs.6.conv.modulation\n",
            "assign unique operator name pnnx_unique_20 to convs.7.conv.modulation\n",
            "assign unique operator name pnnx_unique_21 to to_rgbs.3.conv.modulation\n",
            "assign unique operator name pnnx_unique_22 to convs.8.conv.modulation\n",
            "assign unique operator name pnnx_unique_23 to convs.9.conv.modulation\n",
            "assign unique operator name pnnx_unique_24 to to_rgbs.4.conv.modulation\n",
            "assign unique operator name pnnx_unique_25 to convs.10.conv.modulation\n",
            "assign unique operator name pnnx_unique_26 to convs.11.conv.modulation\n",
            "assign unique operator name pnnx_unique_27 to to_rgbs.5.conv.modulation\n",
            "assign unique operator name pnnx_unique_28 to convs.12.conv.modulation\n",
            "assign unique operator name pnnx_unique_29 to convs.13.conv.modulation\n",
            "assign unique operator name pnnx_unique_30 to to_rgbs.6.conv.modulation\n",
            "assign unique operator name pnnx_unique_31 to convs.14.conv.modulation\n",
            "assign unique operator name pnnx_unique_32 to convs.15.conv.modulation\n",
            "assign unique operator name pnnx_unique_33 to to_rgbs.7.conv.modulation\n",
            "eliminate_noop_math aten::div pnnx_7628\n",
            "eliminate_noop_math aten::div pnnx_7611\n",
            "eliminate_noop_math aten::sub pnnx_7468\n",
            "eliminate_noop_math aten::sub pnnx_7461\n",
            "eliminate_noop_math aten::mul pnnx_7328\n",
            "eliminate_noop_math aten::mul pnnx_7188\n",
            "eliminate_noop_math aten::div pnnx_7105\n",
            "eliminate_noop_math aten::mul pnnx_7091\n",
            "eliminate_noop_math aten::div pnnx_7088\n",
            "eliminate_noop_math aten::mul pnnx_7075\n",
            "eliminate_noop_math aten::mul pnnx_7032\n",
            "eliminate_noop_math aten::mul pnnx_7018\n",
            "eliminate_noop_math aten::mul pnnx_6979\n",
            "eliminate_noop_math aten::mul pnnx_6970\n",
            "eliminate_noop_math aten::sub pnnx_6944\n",
            "eliminate_noop_math aten::sub pnnx_6937\n",
            "eliminate_noop_math aten::mul pnnx_6918\n",
            "eliminate_noop_math aten::mul pnnx_6915\n",
            "eliminate_noop_math aten::mul pnnx_6774\n",
            "eliminate_noop_math aten::div pnnx_6727\n",
            "eliminate_noop_math aten::div pnnx_6710\n",
            "eliminate_noop_math aten::sub pnnx_6567\n",
            "eliminate_noop_math aten::sub pnnx_6560\n",
            "eliminate_noop_math aten::mul pnnx_6427\n",
            "eliminate_noop_math aten::mul pnnx_6287\n",
            "eliminate_noop_math aten::div pnnx_6204\n",
            "eliminate_noop_math aten::mul pnnx_6190\n",
            "eliminate_noop_math aten::div pnnx_6187\n",
            "eliminate_noop_math aten::mul pnnx_6174\n",
            "eliminate_noop_math aten::mul pnnx_6131\n",
            "eliminate_noop_math aten::mul pnnx_6117\n",
            "eliminate_noop_math aten::mul pnnx_6078\n",
            "eliminate_noop_math aten::mul pnnx_6069\n",
            "eliminate_noop_math aten::sub pnnx_6043\n",
            "eliminate_noop_math aten::sub pnnx_6036\n",
            "eliminate_noop_math aten::mul pnnx_6017\n",
            "eliminate_noop_math aten::mul pnnx_6014\n",
            "eliminate_noop_math aten::mul pnnx_5873\n",
            "eliminate_noop_math aten::div pnnx_5826\n",
            "eliminate_noop_math aten::div pnnx_5809\n",
            "eliminate_noop_math aten::sub pnnx_5666\n",
            "eliminate_noop_math aten::sub pnnx_5659\n",
            "eliminate_noop_math aten::mul pnnx_5526\n",
            "eliminate_noop_math aten::mul pnnx_5386\n",
            "eliminate_noop_math aten::div pnnx_5303\n",
            "eliminate_noop_math aten::mul pnnx_5289\n",
            "eliminate_noop_math aten::div pnnx_5286\n",
            "eliminate_noop_math aten::mul pnnx_5273\n",
            "eliminate_noop_math aten::mul pnnx_5230\n",
            "eliminate_noop_math aten::mul pnnx_5216\n",
            "eliminate_noop_math aten::mul pnnx_5177\n",
            "eliminate_noop_math aten::mul pnnx_5168\n",
            "eliminate_noop_math aten::sub pnnx_5142\n",
            "eliminate_noop_math aten::sub pnnx_5135\n",
            "eliminate_noop_math aten::mul pnnx_5116\n",
            "eliminate_noop_math aten::mul pnnx_5113\n",
            "eliminate_noop_math aten::mul pnnx_4972\n",
            "eliminate_noop_math aten::div pnnx_4925\n",
            "eliminate_noop_math aten::div pnnx_4908\n",
            "eliminate_noop_math aten::sub pnnx_4765\n",
            "eliminate_noop_math aten::sub pnnx_4758\n",
            "eliminate_noop_math aten::mul pnnx_4625\n",
            "eliminate_noop_math aten::mul pnnx_4485\n",
            "eliminate_noop_math aten::div pnnx_4402\n",
            "eliminate_noop_math aten::mul pnnx_4388\n",
            "eliminate_noop_math aten::div pnnx_4385\n",
            "eliminate_noop_math aten::mul pnnx_4372\n",
            "eliminate_noop_math aten::mul pnnx_4329\n",
            "eliminate_noop_math aten::mul pnnx_4315\n",
            "eliminate_noop_math aten::mul pnnx_4276\n",
            "eliminate_noop_math aten::mul pnnx_4267\n",
            "eliminate_noop_math aten::sub pnnx_4241\n",
            "eliminate_noop_math aten::sub pnnx_4234\n",
            "eliminate_noop_math aten::mul pnnx_4215\n",
            "eliminate_noop_math aten::mul pnnx_4212\n",
            "eliminate_noop_math aten::mul pnnx_4071\n",
            "eliminate_noop_math aten::div pnnx_4024\n",
            "eliminate_noop_math aten::div pnnx_4007\n",
            "eliminate_noop_math aten::sub pnnx_3864\n",
            "eliminate_noop_math aten::sub pnnx_3857\n",
            "eliminate_noop_math aten::mul pnnx_3724\n",
            "eliminate_noop_math aten::mul pnnx_3584\n",
            "eliminate_noop_math aten::div pnnx_3501\n",
            "eliminate_noop_math aten::mul pnnx_3487\n",
            "eliminate_noop_math aten::div pnnx_3484\n",
            "eliminate_noop_math aten::mul pnnx_3471\n",
            "eliminate_noop_math aten::mul pnnx_3428\n",
            "eliminate_noop_math aten::mul pnnx_3414\n",
            "eliminate_noop_math aten::mul pnnx_3375\n",
            "eliminate_noop_math aten::mul pnnx_3366\n",
            "eliminate_noop_math aten::sub pnnx_3340\n",
            "eliminate_noop_math aten::sub pnnx_3333\n",
            "eliminate_noop_math aten::mul pnnx_3314\n",
            "eliminate_noop_math aten::mul pnnx_3311\n",
            "eliminate_noop_math aten::mul pnnx_3170\n",
            "eliminate_noop_math aten::div pnnx_3123\n",
            "eliminate_noop_math aten::div pnnx_3106\n",
            "eliminate_noop_math aten::sub pnnx_2963\n",
            "eliminate_noop_math aten::sub pnnx_2956\n",
            "eliminate_noop_math aten::mul pnnx_2823\n",
            "eliminate_noop_math aten::mul pnnx_2683\n",
            "eliminate_noop_math aten::div pnnx_2600\n",
            "eliminate_noop_math aten::mul pnnx_2586\n",
            "eliminate_noop_math aten::div pnnx_2583\n",
            "eliminate_noop_math aten::mul pnnx_2570\n",
            "eliminate_noop_math aten::mul pnnx_2527\n",
            "eliminate_noop_math aten::mul pnnx_2513\n",
            "eliminate_noop_math aten::mul pnnx_2474\n",
            "eliminate_noop_math aten::mul pnnx_2465\n",
            "eliminate_noop_math aten::sub pnnx_2439\n",
            "eliminate_noop_math aten::sub pnnx_2432\n",
            "eliminate_noop_math aten::mul pnnx_2413\n",
            "eliminate_noop_math aten::mul pnnx_2410\n",
            "eliminate_noop_math aten::mul pnnx_2269\n",
            "eliminate_noop_math aten::div pnnx_2222\n",
            "eliminate_noop_math aten::div pnnx_2205\n",
            "eliminate_noop_math aten::sub pnnx_2062\n",
            "eliminate_noop_math aten::sub pnnx_2055\n",
            "eliminate_noop_math aten::mul pnnx_1922\n",
            "eliminate_noop_math aten::mul pnnx_1782\n",
            "eliminate_noop_math aten::div pnnx_1699\n",
            "eliminate_noop_math aten::mul pnnx_1685\n",
            "eliminate_noop_math aten::div pnnx_1682\n",
            "eliminate_noop_math aten::mul pnnx_1669\n",
            "eliminate_noop_math aten::mul pnnx_1626\n",
            "eliminate_noop_math aten::mul pnnx_1612\n",
            "eliminate_noop_math aten::mul pnnx_1573\n",
            "eliminate_noop_math aten::mul pnnx_1564\n",
            "eliminate_noop_math aten::sub pnnx_1538\n",
            "eliminate_noop_math aten::sub pnnx_1531\n",
            "eliminate_noop_math aten::mul pnnx_1512\n",
            "eliminate_noop_math aten::mul pnnx_1509\n",
            "eliminate_noop_math aten::mul pnnx_1368\n",
            "eliminate_noop_math aten::div pnnx_1321\n",
            "eliminate_noop_math aten::div pnnx_1304\n",
            "eliminate_noop_math aten::sub pnnx_1161\n",
            "eliminate_noop_math aten::sub pnnx_1154\n",
            "eliminate_noop_math aten::mul pnnx_1021\n",
            "eliminate_noop_math aten::mul pnnx_881\n",
            "eliminate_noop_math aten::div pnnx_798\n",
            "eliminate_noop_math aten::mul pnnx_784\n",
            "eliminate_noop_math aten::div pnnx_781\n",
            "eliminate_noop_math aten::mul pnnx_768\n",
            "eliminate_noop_math aten::mul pnnx_725\n",
            "eliminate_noop_math aten::mul pnnx_711\n",
            "eliminate_noop_math aten::mul pnnx_672\n",
            "eliminate_noop_math aten::mul pnnx_663\n",
            "eliminate_noop_math aten::sub pnnx_637\n",
            "eliminate_noop_math aten::sub pnnx_630\n",
            "eliminate_noop_math aten::mul pnnx_611\n",
            "eliminate_noop_math aten::mul pnnx_608\n",
            "eliminate_noop_math aten::mul pnnx_467\n",
            "eliminate_noop_math aten::mul pnnx_378\n",
            "eliminate_noop_math aten::mul pnnx_237\n",
            "############# pass_level4\n",
            "############# pass_level5\n",
            "############# pass_ncnn\n",
            "fallback batch axis 233 for operand 5\n",
            "fallback batch axis 233 for operand 11\n",
            "fallback batch axis 233 for operand 17\n",
            "fallback batch axis 233 for operand 23\n",
            "fallback batch axis 233 for operand 29\n",
            "fallback batch axis 233 for operand 35\n",
            "fallback batch axis 233 for operand 41\n",
            "fallback batch axis 233 for operand 47\n",
            "fallback batch axis 233 for operand 56\n",
            "fallback batch axis 233 for operand 57\n",
            "fallback batch axis 233 for operand 85\n",
            "fallback batch axis 233 for operand 93\n",
            "fallback batch axis 233 for operand 94\n",
            "fallback batch axis 233 for operand 99\n",
            "fallback batch axis 233 for operand 102\n",
            "fallback batch axis 233 for operand 103\n",
            "fallback batch axis 233 for operand 116\n",
            "fallback batch axis 233 for operand 117\n",
            "fallback batch axis 233 for operand 121\n",
            "fallback batch axis 233 for operand 130\n",
            "fallback batch axis 233 for operand 131\n",
            "fallback batch axis 233 for operand 141\n",
            "fallback batch axis 233 for operand 149\n",
            "fallback batch axis 233 for operand 150\n",
            "fallback batch axis 233 for operand 155\n",
            "fallback batch axis 233 for operand 157\n",
            "fallback batch axis 233 for operand 158\n",
            "fallback batch axis 233 for operand 159\n",
            "fallback batch axis 233 for operand 163\n",
            "fallback batch axis 233 for operand 168\n",
            "fallback batch axis 233 for operand 169\n",
            "fallback batch axis 233 for operand 182\n",
            "fallback batch axis 233 for operand 183\n",
            "fallback batch axis 233 for operand 187\n",
            "fallback batch axis 233 for operand 196\n",
            "fallback batch axis 233 for operand 197\n",
            "fallback batch axis 233 for operand 207\n",
            "fallback batch axis 233 for operand 215\n",
            "fallback batch axis 233 for operand 216\n",
            "fallback batch axis 233 for operand 221\n",
            "fallback batch axis 233 for operand 223\n",
            "fallback batch axis 233 for operand 224\n",
            "fallback batch axis 233 for operand 225\n",
            "fallback batch axis 233 for operand 229\n",
            "fallback batch axis 233 for operand 234\n",
            "fallback batch axis 233 for operand 235\n",
            "fallback batch axis 233 for operand 248\n",
            "fallback batch axis 233 for operand 249\n",
            "fallback batch axis 233 for operand 253\n",
            "fallback batch axis 233 for operand 262\n",
            "fallback batch axis 233 for operand 263\n",
            "fallback batch axis 233 for operand 273\n",
            "fallback batch axis 233 for operand 281\n",
            "fallback batch axis 233 for operand 282\n",
            "fallback batch axis 233 for operand 287\n",
            "fallback batch axis 233 for operand 289\n",
            "fallback batch axis 233 for operand 290\n",
            "fallback batch axis 233 for operand 291\n",
            "fallback batch axis 233 for operand 295\n",
            "fallback batch axis 233 for operand 300\n",
            "fallback batch axis 233 for operand 301\n",
            "fallback batch axis 233 for operand 314\n",
            "fallback batch axis 233 for operand 315\n",
            "fallback batch axis 233 for operand 319\n",
            "fallback batch axis 233 for operand 328\n",
            "fallback batch axis 233 for operand 329\n",
            "fallback batch axis 233 for operand 339\n",
            "fallback batch axis 233 for operand 347\n",
            "fallback batch axis 233 for operand 348\n",
            "fallback batch axis 233 for operand 353\n",
            "fallback batch axis 233 for operand 355\n",
            "fallback batch axis 233 for operand 356\n",
            "fallback batch axis 233 for operand 357\n",
            "fallback batch axis 233 for operand 361\n",
            "fallback batch axis 233 for operand 366\n",
            "fallback batch axis 233 for operand 367\n",
            "fallback batch axis 233 for operand 380\n",
            "fallback batch axis 233 for operand 381\n",
            "fallback batch axis 233 for operand 385\n",
            "fallback batch axis 233 for operand 394\n",
            "fallback batch axis 233 for operand 395\n",
            "fallback batch axis 233 for operand 405\n",
            "fallback batch axis 233 for operand 413\n",
            "fallback batch axis 233 for operand 414\n",
            "fallback batch axis 233 for operand 419\n",
            "fallback batch axis 233 for operand 421\n",
            "fallback batch axis 233 for operand 422\n",
            "fallback batch axis 233 for operand 423\n",
            "fallback batch axis 233 for operand 427\n",
            "fallback batch axis 233 for operand 432\n",
            "fallback batch axis 233 for operand 433\n",
            "fallback batch axis 233 for operand 446\n",
            "fallback batch axis 233 for operand 447\n",
            "fallback batch axis 233 for operand 451\n",
            "fallback batch axis 233 for operand 460\n",
            "fallback batch axis 233 for operand 461\n",
            "fallback batch axis 233 for operand 471\n",
            "fallback batch axis 233 for operand 479\n",
            "fallback batch axis 233 for operand 480\n",
            "fallback batch axis 233 for operand 485\n",
            "fallback batch axis 233 for operand 487\n",
            "fallback batch axis 233 for operand 488\n",
            "fallback batch axis 233 for operand 489\n",
            "fallback batch axis 233 for operand 493\n",
            "fallback batch axis 233 for operand 498\n",
            "fallback batch axis 233 for operand 499\n",
            "fallback batch axis 233 for operand 512\n",
            "fallback batch axis 233 for operand 513\n",
            "fallback batch axis 233 for operand 517\n",
            "fallback batch axis 233 for operand 526\n",
            "fallback batch axis 233 for operand 527\n",
            "fallback batch axis 233 for operand 537\n",
            "fallback batch axis 233 for operand 545\n",
            "fallback batch axis 233 for operand 546\n",
            "fallback batch axis 233 for operand 551\n",
            "fallback batch axis 233 for operand 553\n",
            "fallback batch axis 233 for operand 554\n",
            "fallback batch axis 233 for operand 555\n",
            "fallback batch axis 233 for operand 559\n",
            "fallback batch axis 233 for operand 564\n",
            "fallback batch axis 233 for operand 565\n",
            "fallback batch axis 233 for operand 578\n",
            "fallback batch axis 233 for operand 579\n",
            "fallback batch axis 233 for operand 583\n",
            "fallback batch axis 233 for operand 592\n",
            "fallback batch axis 233 for operand 593\n",
            "fallback batch axis 233 for operand 603\n",
            "fallback batch axis 233 for operand 611\n",
            "fallback batch axis 233 for operand 612\n",
            "fallback batch axis 233 for operand 617\n",
            "fallback batch axis 233 for operand 619\n",
            "fallback batch axis 233 for operand 620\n",
            "fallback batch axis 233 for operand 621\n",
            "fallback batch axis 233 for operand 625\n",
            "unbind along batch axis 0 is not supported\n",
            "reshape tensor with batch index 24 is not supported yet!\n",
            "reshape tensor with batch index 24 is not supported yet!\n",
            "reshape tensor with batch index 24 is not supported yet!\n",
            "reshape tensor with batch index 24 is not supported yet!\n",
            "reshape tensor with batch index 24 is not supported yet!\n",
            "reshape tensor with batch index 24 is not supported yet!\n",
            "reshape tensor with batch index 24 is not supported yet!\n",
            "reshape tensor with batch index 24 is not supported yet!\n",
            "reshape tensor with batch index 24 is not supported yet!\n",
            "reshape tensor with batch index 24 is not supported yet!\n",
            "reshape tensor with batch index 25 is not supported yet!\n",
            "reshape tensor with batch index 25 is not supported yet!\n",
            "reshape tensor with batch index 25 is not supported yet!\n",
            "reshape tensor with batch index 25 is not supported yet!\n",
            "reshape tensor with batch index 25 is not supported yet!\n",
            "reshape tensor with batch index 25 is not supported yet!\n",
            "reshape tensor with batch index 25 is not supported yet!\n",
            "reshape tensor with batch index 25 is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "reshape to 6-rank tensor is not supported yet!\n",
            "ignore Slice unbind_0 param dim=0\n",
            "ignore F.conv_transpose2d F.conv_transposend_34 param bias=None\n",
            "ignore F.conv_transpose2d F.conv_transposend_34 param dilation=(1,1)\n",
            "ignore F.conv_transpose2d F.conv_transposend_34 param groups=1\n",
            "ignore F.conv_transpose2d F.conv_transposend_34 param output_padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_34 param padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_34 param stride=(2,2)\n",
            "ignore F.conv_transpose2d F.conv_transposend_35 param bias=None\n",
            "ignore F.conv_transpose2d F.conv_transposend_35 param dilation=(1,1)\n",
            "ignore F.conv_transpose2d F.conv_transposend_35 param groups=1\n",
            "ignore F.conv_transpose2d F.conv_transposend_35 param output_padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_35 param padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_35 param stride=(2,2)\n",
            "ignore F.conv_transpose2d F.conv_transposend_36 param bias=None\n",
            "ignore F.conv_transpose2d F.conv_transposend_36 param dilation=(1,1)\n",
            "ignore F.conv_transpose2d F.conv_transposend_36 param groups=1\n",
            "ignore F.conv_transpose2d F.conv_transposend_36 param output_padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_36 param padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_36 param stride=(2,2)\n",
            "ignore F.conv_transpose2d F.conv_transposend_37 param bias=None\n",
            "ignore F.conv_transpose2d F.conv_transposend_37 param dilation=(1,1)\n",
            "ignore F.conv_transpose2d F.conv_transposend_37 param groups=1\n",
            "ignore F.conv_transpose2d F.conv_transposend_37 param output_padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_37 param padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_37 param stride=(2,2)\n",
            "ignore F.conv_transpose2d F.conv_transposend_38 param bias=None\n",
            "ignore F.conv_transpose2d F.conv_transposend_38 param dilation=(1,1)\n",
            "ignore F.conv_transpose2d F.conv_transposend_38 param groups=1\n",
            "ignore F.conv_transpose2d F.conv_transposend_38 param output_padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_38 param padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_38 param stride=(2,2)\n",
            "ignore F.conv_transpose2d F.conv_transposend_39 param bias=None\n",
            "ignore F.conv_transpose2d F.conv_transposend_39 param dilation=(1,1)\n",
            "ignore F.conv_transpose2d F.conv_transposend_39 param groups=1\n",
            "ignore F.conv_transpose2d F.conv_transposend_39 param output_padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_39 param padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_39 param stride=(2,2)\n",
            "ignore F.conv_transpose2d F.conv_transposend_40 param bias=None\n",
            "ignore F.conv_transpose2d F.conv_transposend_40 param dilation=(1,1)\n",
            "ignore F.conv_transpose2d F.conv_transposend_40 param groups=1\n",
            "ignore F.conv_transpose2d F.conv_transposend_40 param output_padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_40 param padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_40 param stride=(2,2)\n",
            "ignore F.conv_transpose2d F.conv_transposend_41 param bias=None\n",
            "ignore F.conv_transpose2d F.conv_transposend_41 param dilation=(1,1)\n",
            "ignore F.conv_transpose2d F.conv_transposend_41 param groups=1\n",
            "ignore F.conv_transpose2d F.conv_transposend_41 param output_padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_41 param padding=(0,0)\n",
            "ignore F.conv_transpose2d F.conv_transposend_41 param stride=(2,2)\n"
          ]
        }
      ]
    }
  ]
}